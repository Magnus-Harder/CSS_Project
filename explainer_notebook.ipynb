{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "### Data\n",
    "#### Completed Elements:\n",
    "1. Get Characters and WikiAPi links for characters in the books\n",
    "2. Optain aliases for characters\n",
    "3. Obtain chracter description/overview from  WikiApi\n",
    "\n",
    "#### Missing Elements:\n",
    "1. Format the rest of the books\n",
    "\n",
    "\n",
    "### Network\n",
    "#### Completed Elements:\n",
    "1. Write functions that given af Chapter computed the network of characters in that chapter\n",
    "2. \n",
    "\n",
    "#### Missing Elements:\n",
    "1. Analyse the dynamic network of chracters over the books.\n",
    "2. Compute centrality measures for the characters in the network\n",
    "3. Community detection in the network for each Chapter\n",
    "4. Test  \n",
    "\n",
    "### Text\n",
    "#### Completed Elements:\n",
    "1. Function for character sentiments\n",
    "2. Construct word clouds for character\n",
    "\n",
    "#### Missing Elements:\n",
    "1. Construct word clouds and sentiments for communities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harry Potter Dynamic Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# Text libraries\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Network libraries\n",
    "import networkx as nx\n",
    "import netwulf as nw\n",
    "\n",
    "# Custom libraries\n",
    "from Utils.tokenize_function import stop_tokenize\n",
    "from Utils.get_text import text_book\n",
    "\n",
    "import os\n",
    "os.getcwd(), os.path.exists(\"1.Dataset_files/Books/Book 1 - The Philosopher's Stone.txt\")\n",
    "\n",
    "# Flatten list function\n",
    "def flatten(list_list : list) -> list:\n",
    "    return [item for sublist in list_list for item in sublist]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "1. Get Characters and WikiAPi links for characters in the books\n",
    "2. Optain aliases for characters\n",
    "3. Obtain chracter description/overview from  WikiApi\n",
    "4. Download the books from the web\n",
    "5. Format the rest of the books\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "\n",
    "# Define Links for each books Character lists\n",
    "Books =[\n",
    "    #1\n",
    "    'Harry_Potter_and_the_Philosopher%27s_Stone_(character_index)',\n",
    "    #2\n",
    "    'Harry_Potter_and_the_Chamber_of_Secrets_(character_index)',\n",
    "    #3\n",
    "    'Harry_Potter_and_the_Prisoner_of_Azkaban_(character_index)',\n",
    "    #4\n",
    "    'Harry_Potter_and_the_Goblet_of_Fire_(character_index)',\n",
    "    #5\n",
    "    'Harry_Potter_and_the_Order_of_the_Phoenix_(character_index)',\n",
    "    #6\n",
    "    'Harry_Potter_and_the_Half-Blood_Prince_(character_index)',\n",
    "    #7\n",
    "    'Harry_Potter_and_the_Deathly_Hallows_(character_index)'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get the data\n",
    "\n",
    "def get_data(book, nr):\n",
    "\n",
    "    # Get JSON data from the API\n",
    "    query = requests.get(f'https://harrypotter.fandom.com/api.php?action=parse&page={book}&format=json').json()\n",
    "    \n",
    "    # Get the HTML data from the JSON\n",
    "    HTML = query['parse']['text']['*']\n",
    "\n",
    "    # Parse the HTML data\n",
    "    soup = BeautifulSoup(HTML, 'html.parser')\n",
    "\n",
    "    # Get rows of characters\n",
    "    \n",
    "    if nr < 3:\n",
    "        Chars = soup.find_all('td')\n",
    "    else:\n",
    "        sub_Chars = soup.find_all('ul')[1:-1]\n",
    "        Chars = []\n",
    "        for sub_Char in sub_Chars:\n",
    "            Chars.extend(sub_Char.find_all('li'))\n",
    "\n",
    "    return Chars,soup\n",
    "\n",
    "\n",
    "# Define the function to get name and link of each character\n",
    "\n",
    "def get_character(Char_td):\n",
    "\n",
    "    # Get the name of the character\n",
    "    name = Char_td.find('a').text\n",
    "\n",
    "    # Get the link of the character\n",
    "    link = Char_td.find('a')['href']\n",
    "\n",
    "    return name, link\n",
    "\n",
    "# Define the function to get name, link for each character\n",
    "\n",
    "def get_Char_td(Chars):\n",
    "\n",
    "    # Initialize the list of names and links\n",
    "    Names = []\n",
    "    Links = []\n",
    "\n",
    "    for char in Chars:\n",
    "\n",
    "        # Check if character\n",
    "        if char.find('a') == None:\n",
    "            continue\n",
    "    \n",
    "        if char.find('a').get('class') != None:\n",
    "            continue\n",
    "\n",
    "        # Get the name and link of the character\n",
    "        name, link = get_character(char)\n",
    "\n",
    "        # Append the name and link to the list\n",
    "        Names.append(name)\n",
    "        Links.append(link)\n",
    "\n",
    "    return Names, Links\n",
    "\n",
    "# Define the function to get the data for each book\n",
    "\n",
    "def get_all_data(Books):\n",
    "\n",
    "    # Initialize the list of names and links\n",
    "    Names = []\n",
    "    Links = []\n",
    "\n",
    "    # Get the data for each book\n",
    "    for nr, book in enumerate(Books):\n",
    "        print(f'Getting data for {book}...')\n",
    "        # Get the data for each book\n",
    "        Chars,_ = get_data(book, nr+1)\n",
    "\n",
    "        # Get the name and link of each character\n",
    "        Name, Link = get_Char_td(Chars)\n",
    "\n",
    "        # Append the name and link to the list\n",
    "        Names.append(Name)\n",
    "        Links.append(Link)\n",
    "\n",
    "        # print number of characters and first 5 characters\n",
    "        print(f'Number of characters: {len(Name)}')\n",
    "\n",
    "    #Names = flatten(Names)\n",
    "    #Links = flatten(Links)\n",
    "\n",
    "    return Names, Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for Harry_Potter_and_the_Philosopher%27s_Stone_(character_index)...\n",
      "Number of characters: 155\n",
      "Getting data for Harry_Potter_and_the_Chamber_of_Secrets_(character_index)...\n",
      "Number of characters: 65\n",
      "Getting data for Harry_Potter_and_the_Prisoner_of_Azkaban_(character_index)...\n",
      "Number of characters: 46\n",
      "Getting data for Harry_Potter_and_the_Goblet_of_Fire_(character_index)...\n",
      "Number of characters: 151\n",
      "Getting data for Harry_Potter_and_the_Order_of_the_Phoenix_(character_index)...\n",
      "Number of characters: 129\n",
      "Getting data for Harry_Potter_and_the_Half-Blood_Prince_(character_index)...\n",
      "Number of characters: 83\n",
      "Getting data for Harry_Potter_and_the_Deathly_Hallows_(character_index)...\n",
      "Number of characters: 82\n"
     ]
    }
   ],
   "source": [
    "# Get Names and Links for each book\n",
    "Names, Links = get_all_data(Books)\n",
    "\n",
    "# Save as DataFrame\n",
    "CharacterData = pd.DataFrame({'Name': Names, 'Link': Links})\n",
    "\n",
    "# Save as Excel\n",
    "CharacterData.to_excel('Temp/CharacterData.xlsx', index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "\n",
    "# load the data\n",
    "NnL = pd.read_excel('Temp/CharacterData.xlsx',index_col=0)\n",
    "\n",
    "# Define edge cases\n",
    "edge_aliases = {0: 'Mr. Dursley', 1: 'Mrs. Dursley', 3: 'Mrs. Potter', 5: 'Mr. Potter', 7: 'My Lord',\n",
    "                11: 'Dumbledore', 12: 'McGonagall', 13: 'Diggle', 14: 'Pomfrey',\n",
    "                15: 'Hagrid', 18: 'Mrs. Figg', 21: 'Mr. Paws', 28: 'Boa Constrictor',\n",
    "                37: 'Mr. Evans', 38: 'Mrs. Evans', 53: 'Quirrell', 57: 'Malfoy',\n",
    "                62: 'Mr. Ollivander', 65: 'Mrs. Weasley', 69: 'Mr. Weasley',\n",
    "                70: 'Longbottom', 78: 'Agrippa', 79: 'Ptolemy', 80: 'Grindelwald',\n",
    "                90: 'Miss Granger', 91: 'Crabbe', 92: 'Goyle', 95: 'Friar',\n",
    "                116: 'Nearly Headless Nick', 118: 'Mr. Finnigan', 119: 'Mrs. Finnigan', \n",
    "                121: 'Snape', 122: 'Filch', 123: 'Madam Hooch', 125: 'Mrs. Norris', \n",
    "                126: 'Professor Sprout', 127: 'Professor Binns', 130: 'Professor Flitwick', \n",
    "                132: 'Wood', 139: 'Flint', 140: 'Spinnet', 143: 'Pucey', \n",
    "                144: 'Higgs', 145: 'Madam Pince', 153: 'giant squid', 155: 'Mr. Mason',\n",
    "                156: 'Mrs. Mason', 164: 'Lockhart', 171: 'Mr. Borgin', 173: 'Mr. Granger',\n",
    "                174: 'Mrs. Granger', 175: 'Dr. Filibuster', 177: 'Manager of Flourish and Blotts',\n",
    "                181: 'Mr. Creevey', 187: 'Sir Patrick', 189: 'D. J. Prod', 195: 'Mrs. Skower',\n",
    "                204: 'Macmillan', 206: 'Professor Sinistra', 211: 'Dippet', 213: 'governors', 217: 'Tom',\n",
    "                234: 'Lupin', 235: 'Professor Kettleburn', 238: 'Professor Trelawney',\n",
    "                254: 'Professor Vector', 256: 'Davies', 257: 'Warrington', 258: 'Montague',\n",
    "                259: 'Derrick', 260: 'Bole', 262: 'Macnair', 266: 'Mrs. Riddle', 267: 'Mr. Riddle',\n",
    "                284: 'Krum', 285: 'Mr. Diggory', 289: 'Mr. Roberts', 290: 'Mr. Payne', \n",
    "                296: 'Mr. Wood', 297: 'Mrs. Wood', 301: 'Bode', 302: 'Croaker', 303: 'Pontner',\n",
    "                312: 'Dimitrov', 313: 'Ivanova', 314: 'Zograf', 315: 'Levski', 316: 'Vulchanov',\n",
    "                317: 'Volkov', 319: 'Ryan', 324: 'Lynch', 325: 'Mostafa', 326: 'Mrs. Roberts',\n",
    "                329: 'Madam Maxime', 332: 'Skeeter', 333: 'Mad-Eye', 346: 'Master Barty',\n",
    "                350: 'Karkaroff', 356: 'Gregorovitch', 369: 'Professor Grubbly-Plank', 381: 'Rosier',\n",
    "                386: 'Mrs. Fudge', 388: 'Dolohov', 390: 'Rookwood', 395: \"Frank Longbottom's wife\",\n",
    "                399: 'Mrs. Diggory', 400: 'Mr. Krum', 401: 'Mrs. Krum', 418: 'Mr. Prentice',\n",
    "                420: 'Shacklebolt', 423: 'Podmore', 447: 'Scrimgeour', 451: 'Umbridge',\n",
    "                452: 'Slinkhard', 464: 'Mr. Lovegood', 478: 'Mrs. Chang', 489: 'Barnabas the Barmy', 490: 'Mr. Edgecombe',\n",
    "                491: 'Mr. Chang', 496: 'Kirke', 497: 'Slope', 501: 'Fortescu',\n",
    "                506: 'Healer Smethwyck', 507: 'Pye', 511: 'Healer Strout', 518: 'Dawlish',\n",
    "                519: 'Mr. Montague', 520: 'Mrs. Montague', 526: 'Carmichael', 527: 'Dingle',\n",
    "                539: 'Dr. Ubbly', 541: 'Mrs. Goyle', 548: 'Yaxley', 549: 'Carrows', 550: 'Carrows',\n",
    "                551: 'Greyback', 552: 'Slughorn', 562: 'McLaggen', 563: 'Belby',\n",
    "                565: 'Mr. Belby', 575: 'Ogden', 576: 'Mr. Gaunt', 579: 'Mr. Patil',\n",
    "                580: 'Mrs. Patil', 581: 'Mr. Midgen', 582: 'Mrs. Abbott', 583: 'Robins',\n",
    "                584: 'Peakes', 585: 'Coote', 586: 'Bobbin', 588: 'Burkes',\n",
    "                589: 'Mrs. Cole', 590: 'Stubbs', 591: 'Whalley', 592: 'Benson', 593: 'Bishop',\n",
    "                603: 'Professor Merrythought', 607: 'Twycross', 622: 'Bungs', 626: 'Thicknesse',\n",
    "                627: 'Professor Burbage', 627: 'Gorgovitch', 652: 'Cattermole', 653: 'Runcorn',\n",
    "                669: 'youngest brother', 680: 'oldest brother'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'parse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m char_idx \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(NnL))):\n\u001b[1;32m      6\u001b[0m     query \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://harrypotter.fandom.com/api.php?action=parse&page=\u001b[39m\u001b[39m{\u001b[39;00mNnL[\u001b[39m'\u001b[39m\u001b[39mLink\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[char_idx]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m#\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m&format=json\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mjson()\n\u001b[0;32m----> 7\u001b[0m     HTML \u001b[39m=\u001b[39m query[\u001b[39m'\u001b[39;49m\u001b[39mparse\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(HTML, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[39m# Get aliases\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'parse'"
     ]
    }
   ],
   "source": [
    "texts = np.zeros(len(NnL), dtype=object)\n",
    "tokens = np.zeros(len(NnL), dtype=object)\n",
    "aliases = np.zeros(len(NnL), dtype=object)\n",
    "\n",
    "for char_idx in tqdm(range(len(NnL))):\n",
    "    query = requests.get(f\"https://harrypotter.fandom.com/api.php?action=parse&page={NnL['Link'].iloc[char_idx].split('/')[-1].split('#')[0]}&format=json\").json()\n",
    "    HTML = query['parse']['text']['*']\n",
    "    soup = BeautifulSoup(HTML, 'html.parser')\n",
    "\n",
    "    # Get aliases\n",
    "    try:\n",
    "        # Character has aliases\n",
    "        alias_ = soup.find(string='Also known as').findNext(\"div\")\n",
    "        if alias_.find_all('li') != []:\n",
    "            # Character has multiple aliases\n",
    "            alias_ = [i.get_text() for i in alias_.find_all('li')]\n",
    "            alias_ = [re.split('\\s?(\\()|(\\[)',j)[0] for j in alias_]\n",
    "        else:\n",
    "            # Character has only one alias\n",
    "            alias_ = [re.split('\\s?(\\()|(\\[)',alias_.get_text())[0]]\n",
    "    except:\n",
    "        # Character has no aliases\n",
    "        alias_ = []\n",
    "\n",
    "    # Ensure that aliases are not the same as a character name\n",
    "    alias_ = [i for i in alias_ if not NnL['Name'].str.contains(f'{i}', regex=False).any()]\n",
    "\n",
    "    # Add edge cases\n",
    "    if char_idx in edge_aliases:\n",
    "        alias_.append(edge_aliases[char_idx])\n",
    "    \n",
    "    # Add first name to aliases\n",
    "    none_aliases = ['A', 'Mr', 'Mrs', 'Dr', 'Manager', 'The', 'Father','Sorting', 'wizard', 'Chancellor', 'Workmen', 'Waitress', 'Sir',\n",
    "                    'Nearly-Headless', 'Fat','Aged', 'Blood-Sucking', 'Forbidden','Unidentified', 'Zoo', 'Kepper', 'Muggle', 'Muggle-Born',\n",
    "                    'Senior', 'Junior', 'Board', 'Committee','Academy', 'Ministry', 'Department',\n",
    "                    'Little', 'Great', 'Old', 'Young', 'Head', 'Headmaster', 'Headmistress','Weird','Care', 'Montgomery', 'Hogwarts', 'Frank']\n",
    "    if ((NnL['Name'].iloc[char_idx].split(' ')[0] not in none_aliases) and\n",
    "    (\"'s\" not in NnL['Name'].iloc[char_idx]) and \n",
    "    (\"s'\" not in NnL['Name'].iloc[char_idx]) and\n",
    "    (\"family\" not in NnL['Name'].iloc[char_idx]) and\n",
    "    (NnL['Name'].iloc[char_idx].split(' ')[0].isalpha())):\n",
    "        alias_.append(NnL['Name'].iloc[char_idx].split(' ')[0])\n",
    "\n",
    "    # Ensure that aliases are unique and not the same as a character name\n",
    "    alias_ = list(set(alias_))\n",
    "    alias_ = [i for i in alias_ if i != NnL['Name'].iloc[char_idx]]\n",
    "\n",
    "    # Save aliases\n",
    "    aliases[char_idx] = alias_\n",
    "\n",
    "    temp = [i.get_text() for i in soup.find_all('p', class_ = None)]\n",
    "\n",
    "    # Filter out unwanted text\n",
    "    if 'At least some content in this article' in temp[0]:\n",
    "        temp = temp[1:]\n",
    "    if 'At least some content in this article' in temp[0]:\n",
    "        temp = temp[1:]\n",
    "    if 'The title of this article is conjectural.' in temp[0]:\n",
    "        temp = temp[1:]\n",
    "    if 'This entry needs to be cleaned up to conform to a higher standard of quality.' in temp[0]:\n",
    "        temp = temp[1:]\n",
    "    if 'The topic of this article is of a real-life subject' in temp[0]:\n",
    "        temp = temp[1:]\n",
    "    if 'The title of this article intentionally uses incorrect spelling or grammar,' in temp[0]:\n",
    "        temp = temp[1:]\n",
    "    if 'This page \"shows not\" an image, but just a written article.' in temp[0]:\n",
    "        temp = temp[1:]\n",
    "    if 'Point me!' in temp[0]:\n",
    "        temp = temp[1:]\n",
    "    if 'Point me!' in temp[0]:\n",
    "        temp = temp[1:]\n",
    "    if 'At least some content in this article' in temp[0]:\n",
    "        temp = temp[1:]\n",
    "    if 'The title of this article is conjectural.' in temp[0]:\n",
    "        temp = temp[1:]\n",
    "    if 'Biographical information' in temp[0]:\n",
    "        temp = temp[1:]\n",
    "\n",
    "    # Get text    \n",
    "    texts[char_idx] = ' '.join(temp)\n",
    "    tokens[char_idx] = stop_tokenize(texts[char_idx])\n",
    "\n",
    "# Save\n",
    "NnL['WikiText'] = texts\n",
    "NnL['Tokens'] = tokens\n",
    "NnL['Aliases'] = aliases\n",
    "\n",
    "NnL.to_csv(r'Temp/CharacterWikis.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pot_dub.txt\",\"r\") as f:\n",
    "    pot_dub = f.readlines()\n",
    "    pot_dub = [char.strip() for char in pot_dub]\n",
    "    remove_char = []\n",
    "    for char in pot_dub:\n",
    "        if char[-3:] == \"(R)\":\n",
    "            remove_char.append(char[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707\n"
     ]
    }
   ],
   "source": [
    "# Load the character dataset\n",
    "character_df = pd.read_csv(\"1.Dataset_files/CharacterWikis.csv\")\n",
    "character_df.Aliases = [eval(char) for char in character_df.Aliases]\n",
    "character_df.Tokens = [eval(t) for t in character_df.Tokens]\n",
    "character_df.Name[697] = \"Lily L. Potter\"\n",
    "print(len(character_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicorn\n",
      "Weasley family\n",
      "Weasley family\n",
      "Ogden\n",
      "Care of Magical Creatures\n",
      "Unicorn\n",
      "Care of Magical Creatures\n",
      "Hedge Maze\n",
      "Hedge Maze\n",
      "Hedge Maze\n",
      "Crabbe\n",
      "Goyle\n",
      "Nott\n",
      "12 Grimmauld Place\n",
      "12 Grimmauld Place\n",
      "12 Grimmauld Place\n",
      "Bob\n",
      "Care of Magical Creatures\n",
      "Arnold\n",
      "Tiberius\n",
      "Urquhart\n",
      "Lestrange\n",
      "Bletchley\n",
      "Snatchers\n",
      "Dudley\n",
      "Snatchers\n",
      "Ravenclaw\n",
      "Lily L. Potter\n",
      "James Potter II\n",
      "Albus Potter\n"
     ]
    }
   ],
   "source": [
    "# Remove the rows with characters\n",
    "for idx,row in character_df.iterrows():\n",
    "    if row.Name in remove_char:\n",
    "        print(row.Name)\n",
    "        character_df.drop(idx,inplace=True)\n",
    "        continue\n",
    "\n",
    "# Reset the index\n",
    "character_df.index = np.arange(len(character_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the aliases\n",
    "# If alias is a name of a character, remove it\n",
    "Names = character_df.Name.tolist()\n",
    "for idx,row in character_df.iterrows():\n",
    "    for alias in row.Aliases:\n",
    "        if alias in Names:\n",
    "            character_df.at[idx,\"Aliases\"].remove(alias)\n",
    "\n",
    "# Readd the names to the aliases\n",
    "for idx,row in character_df.iterrows():\n",
    "    if row.Name not in row.Aliases:\n",
    "        character_df.at[idx,\"Aliases\"].append(row.Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse traverse the aliases to remove duplicates\n",
    "for i in range(len(character_df)-1,-1,-1):\n",
    "    row = character_df.iloc[i]\n",
    "\n",
    "    # Get all the aliases before the current row\n",
    "    aliases = []\n",
    "    for alias in character_df['Aliases'][:i]:\n",
    "        aliases.extend(alias)\n",
    "\n",
    "    # Remove the duplicates\n",
    "    for alias in row.Aliases:\n",
    "        if alias in aliases:\n",
    "            character_df.at[i,\"Aliases\"].remove(alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_df.to_csv(\"Temp/CharacterWikis_clean.csv\",index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Download Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/magnusharder/Documents/UNI-DTU/6. Semester/Social Sciences/CSS_Project',\n",
       " False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd(), os.path.exists(\"Temp/Books/Book 1 - The Philosopher's Stone.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "path = 'https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/'\n",
    "\n",
    "# Books list\n",
    "books = [\"Book 1 - The Philosopher's Stone.txt\",\n",
    "         \"Book 2 - The Chamber of Secrets.txt\",\n",
    "         \"Book 3 - The Prisoner of Azkaban.txt\",\n",
    "         \"Book 4 - The Goblet of Fire.txt\",\n",
    "         \"Book 5 - The Order of the Phoenix.txt\",\n",
    "         \"Book 6 - The Half Blood Prince.txt\",\n",
    "         \"Book 7 - The Deathly Hallows.txt\"]\n",
    "\n",
    "for book in books:\n",
    "    # Get the book\n",
    "    url = path + book.replace(' ', '%20')\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Save the book\n",
    "    with open('1.Dataset_files/OriginalBooks/'+ book, 'w') as f:\n",
    "        f.write(soup.text)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Split each book into chapters/pages/sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_chapter(Book):\n",
    "    split_string = \"\\n{9}|\\n{8}|\\n{7}|\\n{6}|\\n{5}[0-9]+\\s?\\n{5}\"\n",
    "    Chapters_text = {idx+1 : chapter for idx, chapter in enumerate(re.split(split_string, Book[7:]))}\n",
    "\n",
    "    return Chapters_text\n",
    "\n",
    "\n",
    "def split_pages(Chapter,split_string):\n",
    "    # split_string = \"Page \\| [0-9]+ Harry Potter and the Philosophers Stone -\\s?J.K. Rowling\"\n",
    "    Pages_text = re.split(split_string,Chapter)\n",
    "    return Pages_text\n",
    "\n",
    "def format_page(Page):\n",
    "    Page = re.sub(\"\\n+|\\t\",\"\",Page)\n",
    "    Page = tokenize.sent_tokenize(Page)\n",
    "    return Page\n",
    "\n",
    "\n",
    "def split_book(Book,split_string):\n",
    "    Chapters_text = split_chapter(Book)\n",
    "    for chapter in Chapters_text:\n",
    "        Pages_text = split_pages(Chapters_text[chapter],split_string)\n",
    "        Pages_text = [format_page(Page) for Page in Pages_text[:-1]]\n",
    "        Chapters_text[chapter] = Pages_text\n",
    "\n",
    "    for chapter in Chapters_text:\n",
    "        for idx,page in enumerate(Chapters_text[chapter]):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            else:\n",
    "                if not page[0][0].isupper():\n",
    "\n",
    "                    Chapters_text[chapter][idx-1][-1] = Chapters_text[chapter][idx-1][-1] + \" \" + page[0]\n",
    "                    \n",
    "                    Chapters_text[chapter][idx] = page[1:]\n",
    "                \n",
    "    return Chapters_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Links for each book\n",
    "Books = [\n",
    "    #1\n",
    "    \"Book 1 - The Philosopher's Stone.txt\",\n",
    "    #2\n",
    "    'Book 2 - The Chamber of Secrets.txt',\n",
    "    #3\n",
    "    'Book 3 - The Prisoner of Azkaban.txt',\n",
    "    #4\n",
    "    'Book 4 - The Goblet of Fire.txt',\n",
    "    #5\n",
    "    'Book 5 - The Order of the Phoenix.txt',\n",
    "    #6\n",
    "    'Book 6 - The Half Blood Prince.txt',\n",
    "    #7\n",
    "    'Book 7 - The Deathly Hallows.txt'\n",
    "]\n",
    "\n",
    "split_string = [\n",
    "    \"Page \\| [0-9]+ Harry Potter and the Philosophers Stone -\\s?J.K. Rowling\",\n",
    "    \"Page \\| [0-9]+ Harry Potter and the Chamber of Secrets -\\s?J.K. Rowling\",\n",
    "    \"Page \\| [0-9]+ Harry Potter and the Prisoner of Azkaban -\\s?J.K. Rowling\",\n",
    "    \"P\\s?a\\s?g\\s?e\\s?\\|\\s?[0-9]+\\s?\\n*\\s?Harry Potter and the Goblet of Fire -\\s?J.K. Rowling\",\n",
    "    \"P\\s?a\\s?g\\s?e\\s?\\|\\s?[0-9lUO]+\\s?\\n*\\s*Harry Potter and the Order of the Phoenix -\\s?J.K. Rowling\",\n",
    "    \"P\\s?a\\s?g\\s?e\\s?\\|\\s?[0-9lUO]+\\s?\\n*\\s*Harry Potter and the Half Blood Prince -\\s?J.K. Rowling\",\n",
    "    \"P\\s?a\\s?g\\s?e\\s?\\|\\s?[0-9lUO]+\\s?\\n*\\s*Harry Potter and the Deathly Hallows -\\s?J.K. Rowling\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 1 has: 347 Pages\n",
      "Book 1 has: 17 Chapters\n",
      "Book 2 has: 379 Pages\n",
      "Book 2 has: 18 Chapters\n",
      "Book 3 has: 486 Pages\n",
      "Book 3 has: 22 Chapters\n",
      "Book 4 has: 810 Pages\n",
      "Book 4 has: 37 Chapters\n",
      "Book 5 has: 1107 Pages\n",
      "Book 5 has: 38 Chapters\n",
      "Book 6 has: 729 Pages\n",
      "Book 6 has: 30 Chapters\n",
      "Book 7 has: 855 Pages\n",
      "Book 7 has: 37 Chapters\n"
     ]
    }
   ],
   "source": [
    "for idx,(book_file,split) in enumerate(zip(Books,Names)):\n",
    "    # Read the data\n",
    "    with open(f\"1.Dataset_files/OriginalBooks/{book_file}\", encoding=\"utf8\") as f:\n",
    "        book_text = f.read()\n",
    "\n",
    "    Chapters_text = split_book(book_text,split_string=split)\n",
    "\n",
    "    \n",
    "    print(f\"Book {idx+1} has:\",sum(map(len,Chapters_text.values())),\"Pages\")\n",
    "    print(f\"Book {idx+1} has:\",len(Chapters_text),\"Chapters\")\n",
    "\n",
    "    with open(f\"Temp/Books formatted/Book{idx+1}.pkl\",\"wb\") as f:\n",
    "        pkl.dump(Chapters_text,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of characters: 677, number of aliases: 2000\n"
     ]
    }
   ],
   "source": [
    "# Load characters wiki\n",
    "character_df = pd.read_csv(\"1.Dataset_files/CharacterWikis_clean.csv\")\n",
    "character_df.Aliases = [eval(char) for char in character_df.Aliases]\n",
    "character_df.Tokens = [eval(t) for t in character_df.Tokens]\n",
    "\n",
    "# dict so we have all nodes (list of dicts of all character names (str) and their aliases (list of str)))\n",
    "characters = []\n",
    "for i, char in enumerate(character_df.Aliases):\n",
    "\n",
    "    characters.append({'name':character_df['Name'].iloc[i], 'aliases': char + [character_df['Name'].iloc[i]]})\n",
    "\n",
    "print(f\" Number of characters: {len(characters)}, number of aliases: {sum([len(char['aliases']) for char in characters])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 1, Number of chapters: 17, number of pages: 347, number of sentences: 4929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding character nodes to graph...: 100%|██████████| 677/677 [00:00<00:00, 1094.18it/s]\n",
      "Adding edges to graph...: 100%|██████████| 677/677 [03:30<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 2, Number of chapters: 18, number of pages: 379, number of sentences: 5317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding character nodes to graph...: 100%|██████████| 677/677 [00:00<00:00, 1001.37it/s]\n",
      "Adding edges to graph...: 100%|██████████| 677/677 [03:53<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 3, Number of chapters: 22, number of pages: 486, number of sentences: 7254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding character nodes to graph...: 100%|██████████| 677/677 [00:00<00:00, 785.94it/s]\n",
      "Adding edges to graph...: 100%|██████████| 677/677 [05:00<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 4, Number of chapters: 37, number of pages: 810, number of sentences: 11845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding character nodes to graph...: 100%|██████████| 677/677 [00:01<00:00, 442.24it/s]\n",
      "Adding edges to graph...: 100%|██████████| 677/677 [08:50<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 5, Number of chapters: 38, number of pages: 1107, number of sentences: 12860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding character nodes to graph...: 100%|██████████| 677/677 [00:01<00:00, 343.32it/s]\n",
      "Adding edges to graph...: 100%|██████████| 677/677 [11:32<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 6, Number of chapters: 30, number of pages: 729, number of sentences: 9103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding character nodes to graph...: 100%|██████████| 677/677 [00:01<00:00, 518.67it/s]\n",
      "Adding edges to graph...: 100%|██████████| 677/677 [31:51<00:00,  2.82s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 7, Number of chapters: 37, number of pages: 855, number of sentences: 11024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding character nodes to graph...: 100%|██████████| 677/677 [00:01<00:00, 441.26it/s]\n",
      "Adding edges to graph...: 100%|██████████| 677/677 [08:57<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from Utils.network_utils import get_node_size, get_edge_book_weight\n",
    "\n",
    "Boook_networks = {}\n",
    "\n",
    "for book_nr in range(1,8):\n",
    "    # Load formatted book (dict of chapters with list of lists of pages and sentences)\n",
    "    with open(f\"1.Dataset_files/Books_formatted/Book{book_nr}.pkl\",\"rb\") as f:\n",
    "        book = pkl.load(f)\n",
    "\n",
    "    print(f\"Book {book_nr}, Number of chapters: {len(book)}, number of pages: {sum([len(chapter) for chapter in book.values()])}, number of sentences: {sum([sum([len(page) for page in chapter]) for chapter in book.values()])}\")\n",
    "\n",
    "\n",
    "    # Draw network\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes to graph\n",
    "    for character in tqdm(characters, desc='Adding character nodes to graph...'):\n",
    "        character['sizes'] = get_node_size(character['aliases'], book)\n",
    "        G.add_node(character['name'], sizes=character['sizes'], attr=character['aliases'])\n",
    "\n",
    "    # Add edges to graph\n",
    "    for i, char1 in enumerate(tqdm(characters, desc='Adding edges to graph...')):\n",
    "        for j, char2 in enumerate(characters[i+1:]):\n",
    "            edge_weight = get_edge_book_weight(char1['aliases'], char2['aliases'], book)\n",
    "            if sum(edge_weight.values()) != 0:\n",
    "                G.add_edge(char1['name'], char2['name'], weight=edge_weight)\n",
    "\n",
    "    # Save graph\n",
    "    Boook_networks[book_nr] = G.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"Temp/Book_networks.pkl\",\"wb\") as f:\n",
    "    pkl.dump(Boook_networks,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"Temp/Book_networks.pkl\",\"rb\") as f:\n",
    "    Boook_networks = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 677/677 [03:28<00:00,  3.25it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Characters_dynamic_text = {}\n",
    "\n",
    "for char in tqdm(characters):\n",
    "    for book_nr in range(1,8):\n",
    "        \n",
    "        Char_book_text = {}\n",
    "        \n",
    "        # Load formatted book (dict of chapters with list of lists of pages and sentences)\n",
    "        with open(f\"1.Dataset_files/Books_formatted/Book{book_nr}.pkl\",\"rb\") as f:\n",
    "            book = pkl.load(f)\n",
    "        \n",
    "        Char_book_text[book_nr] = text_book(char['aliases'],book)\n",
    "\n",
    "    Characters_dynamic_text[char['name']] = Char_book_text\n",
    "\n",
    "with open(f\"Temp/Dynamic_text.pkl\",\"wb\") as f:\n",
    "    pkl.dump(Characters_dynamic_text,f)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02467",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
