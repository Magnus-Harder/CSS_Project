{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing libraries\n",
    "import re\n",
    "from nltk import tokenize\n",
    "\n",
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "\n",
    "# File management libraries\n",
    "import pickle as pkl\n",
    "\n",
    "# Web scraping libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Looping utilities\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and preprocessing choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning of the books and character lists\n",
    "\n",
    "We need to reflect on the data-cleaning choices we made in the context of what we wanted to achieve: Namely to build a character network for each chapter in each book, where the nodes were characters, their size were proportional to the number of times they appeared in the chapter, and the edges were weighted by the number of times two characters appeared on the same page in that chapter. Thus, we needed the books to be split to individual sentences, for us to check the interactions between characters on the same page. We also needed the character lists to be cleaned, so that we could check whether a character was mentioned in a sentence and ensure that all relevant character aliases were included in the character lists.\n",
    "\n",
    "**1. Cleaning the book texts:**\n",
    "* Separate the text into chapters by splitting using the chapter headings. Here we used a regex statement to find the chapter headings, but we could also have used the chapter headings from the table of contents, due to spelling mistakes in the chapter headings. The regex statement for the chapter headings was `\"\\n{9}|\\n{8}|\\n{7}|\\n{6}|\\n{5}[0-9]+\\s?\\n{5}\"`. For more details, see the `split_chapters()` function below.\n",
    "* Separate the text into pages by splitting using the page headings. The regex statement for the page headings was `f\"Page \\| [0-9]+ {book_title} -\\s?J.K. Rowling\"`. For more details, see the `split_pages()` function below.\n",
    "* Separate the page text into sentences by using the `nltk` sentence tokenizer to get a list of sentences for a specific page. For more details, see the `format_page()` function below.\t\n",
    "* Create a dictionary where the keys are the chapter numbers and the values are the nested list of nested lists of text on a specific chapter page sentence. The first sentence of a new page was moved to the previous page, as there was a big chance that this was in fact a part of the last sentence on the previous side. The final function to preprocess the books is thus called using the `book_dict = split_book()` function as seen below. (We realize that this sentence is slightly confusing, but this translates to calling the 10th sentence of the 5th page of the 3rd chapter as `book_dict[3][4][9]`)\n",
    "* For each book, the `book_dict = split_book()`function was called, and the resulting dictionary was saved as a pickle file. Before saving, it was tested that each `book_dict` had the right number of chapters and pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chapter(Book):\n",
    "    split_string = \"\\n{9}|\\n{8}|\\n{7}|\\n{6}|\\n{5}[0-9]+\\s?\\n{5}\"\n",
    "    Chapters_text = {idx+1 : chapter for idx, chapter in enumerate(re.split(split_string, Book[7:]))}\n",
    "\n",
    "    return Chapters_text\n",
    "\n",
    "def split_pages(Chapter,split_string):\n",
    "    Pages_text = re.split(split_string,Chapter)\n",
    "    return Pages_text\n",
    "\n",
    "def format_page(Page):\n",
    "    Page = re.sub(\"\\n+|\\t\",\"\",Page)\n",
    "    Page = tokenize.sent_tokenize(Page)\n",
    "    return Page\n",
    "\n",
    "def split_book(Book,split_string):\n",
    "    Chapters_text = split_chapter(Book)\n",
    "    for chapter in Chapters_text:\n",
    "        Pages_text = split_pages(Chapters_text[chapter],split_string)\n",
    "        Pages_text = [format_page(Page) for Page in Pages_text[:-1]]\n",
    "        Chapters_text[chapter] = Pages_text\n",
    "\n",
    "    for chapter in Chapters_text:\n",
    "        for idx,page in enumerate(Chapters_text[chapter]):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            else:\n",
    "                if not page[0][0].isupper():\n",
    "\n",
    "                    Chapters_text[chapter][idx-1][-1] = Chapters_text[chapter][idx-1][-1] + \" \" + page[0]\n",
    "                    \n",
    "                    Chapters_text[chapter][idx] = page[1:]\n",
    "                \n",
    "    return Chapters_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Cleaning the character list:**\n",
    "* The raw character list consisted of a data-frame that included their name, and links. For starters we used the `get_aliases()` function on each character to get a list of the characters webscraped aliases.\n",
    "* This list of aliases was further enhanced by handling some edge-cases: 1) including edge_case aliases (most commonly the most important characters formal name, i.e. \"Mr Potter\", or \"Professor Dumbledore). This process was done by using the `get_edge_alias()` function and linked to a manually curated list of additional aliases that we knew were present in the books, but had been left out of the fandom alias section. 2) including their first name as an alias. This was done using the `add_first_name()` function, and again included some edge cases that where handled. These edge cases typically centered around the characters first name being a descriptive word such as \"Mr.\" and \"Mrs\" instead of an actual name.\n",
    "* Next we focused on cleaning the data further by removing duplicate characters. We manually curated the list `potential_dublicates.txt` that contained the names of all the characters where one of their aliases was the same as another characters name. We then used the `remove_dublicates()` function to remove these characters from the list. There are several reasons for dublicate character names, two major once being 1) that smaller sub-characters in the fandom wiki did not have their separate wiki-pages, but where included into the same overarching page, and that especially Harrys kids in the last book are named after his parents and other important characters. This design choice was made because we didn't want the character graphs and dynamic text later to ascribe the presence of a character to two different characters. This further meant that we excluded some characters, mainly the small characters and Harrys kids, from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_alias(character: str = 'Harry Potter', alias_list: list = []):\n",
    "    with open(\"Temp/character_aliases_edgecases.pkl\", \"rb\") as f:\n",
    "        edge_aliases = pkl.load(f)\n",
    "\n",
    "    if character in edge_aliases.keys():\n",
    "        alias_list.append(edge_aliases[character])\n",
    "    \n",
    "    return alias_list\n",
    "\n",
    "def add_first_name(character: str = 'Harry Potter', alias_list: list = []):\n",
    "    none_aliases = ['A', 'Mr', 'Mrs', 'Dr', 'Manager', 'The', 'Father','Sorting', 'wizard', 'Chancellor', 'Workmen', 'Waitress', 'Sir',\n",
    "                    'Nearly-Headless', 'Fat','Aged', 'Blood-Sucking', 'Forbidden','Unidentified', 'Zoo', 'Kepper', 'Muggle', 'Muggle-Born',\n",
    "                    'Senior', 'Junior', 'Board', 'Committee','Academy', 'Ministry', 'Department',\n",
    "                    'Little', 'Great', 'Old', 'Young', 'Head', 'Headmaster', 'Headmistress','Weird','Care', 'Montgomery', 'Hogwarts', 'Frank']\n",
    "    \n",
    "    none_name = [\"'s\", \"s'\", \"family\"]\n",
    "\n",
    "    if ((character.split(' ')[0] not in none_aliases) and (all([s not in character for s in none_name])) and (character.split(' ')[0].isalpha())):\n",
    "        alias_list.append(character.split(' ')[0])\n",
    "    \n",
    "    return alias_list\n",
    "\n",
    "def get_aliases(character: str = 'Harry Potter', link: str = '/wiki/Harry_Potter', df: pd.DataFrame = NnL):\n",
    "    query = requests.get(f\"https://harrypotter.fandom.com/api.php?action=parse&page={link.split('/')[-1].split('#')[0]}&format=json\").json()\n",
    "    HTML = query['parse']['text']['*']\n",
    "    soup = BeautifulSoup(HTML, 'html.parser')\n",
    "\n",
    "    # Get aliases\n",
    "    try:\n",
    "        # Character has aliases\n",
    "        alias_ = soup.find(string='Also known as').findNext(\"div\")\n",
    "        if alias_.find_all('li') != []:\n",
    "            # Character has multiple aliases\n",
    "            alias_ = [i.get_text() for i in alias_.find_all('li')]\n",
    "            alias_ = [re.split('\\s?(\\()|(\\[)',j)[0] for j in alias_]\n",
    "        else:\n",
    "            # Character has only one alias\n",
    "            alias_ = [re.split('\\s?(\\()|(\\[)',alias_.get_text())[0]]\n",
    "    except:\n",
    "        # Character has no aliases\n",
    "        alias_ = []\n",
    "\n",
    "    # Ensure that aliases are not the same as a character name\n",
    "    alias_ = [i for i in alias_ if not df['Name'].str.contains(f'{i}', regex=False).any()]\n",
    "\n",
    "    # Add edge cases and first name\n",
    "    alias_ = get_edge_alias(character = character, alias_list = alias_)\n",
    "    alias_ = add_first_name(character = character, alias_list = alias_)\n",
    "\n",
    "    # Ensure that aliases are unique and not the same as the character's name\n",
    "    alias_ = list(set(alias_))\n",
    "    alias_ = [i for i in alias_ if i != character]\n",
    "\n",
    "    return alias_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dublicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/potential_dublicates.txt\",\"r\") as f:\n",
    "    pot_dub = f.readlines()\n",
    "    pot_dub = [char.strip() for char in pot_dub]\n",
    "    remove_char = []\n",
    "    for char in pot_dub:\n",
    "        if char[-3:] == \"(R)\":\n",
    "            remove_char.append(char[:-4])\n",
    "\n",
    "# Load the character dataset\n",
    "character_df = pd.read_csv(\"1.Dataset_files/CharacterWikis.csv\")\n",
    "character_df.Aliases = [eval(char) for char in character_df.Aliases]\n",
    "character_df.Tokens = [eval(t) for t in character_df.Tokens]\n",
    "character_df.Name[697] = \"Lily L. Potter\"\n",
    "print(len(character_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial character list has 707 characters, and 1392 aliases in total.\n",
      "The final character list after data cleaning has 677 characters, and 1323 aliases in total.\n"
     ]
    }
   ],
   "source": [
    "# Load formatted book (dict of chapters with list of lists of pages and sentences)\n",
    "initial_character_list = pd.read_csv(\"1.Dataset_files/CharacterWikis.csv\")\n",
    "final_character_list = pd.read_csv(\"Temp/CharacterWikis_clean.csv\")\n",
    "\n",
    "print(f\"The initial character list has {len(initial_character_list)} characters, and {sum([len(eval(l)) for l in initial_character_list['Aliases']]) + len(initial_character_list)} aliases in total.\")\n",
    "print(f\"The final character list after data cleaning has {len(final_character_list)} characters, and {sum([len(eval(l)) for l in final_character_list['Aliases']])} aliases in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initial_character_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m initial_character_list\n",
      "\u001b[1;31mNameError\u001b[0m: name 'initial_character_list' is not defined"
     ]
    }
   ],
   "source": [
    "initial_character_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(eval(l)) for l in pd.read_csv(\"1.Dataset_files/CharacterWikis.csv\")['Aliases']] + len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write about your choices in data cleaning and preprocessing\n",
    "Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02467",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
